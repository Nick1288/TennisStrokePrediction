{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.stats import zscore\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for jupter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\gushi\\LTU\\TennisStrokePrediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gushi\\AppData\\Local\\Temp\\ipykernel_72268\\2372044445.py:5: DtypeWarning: Columns (7,12,24,25,26,31,32,33,34,35,45,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['match_id', 'Pt', 'Set1', 'Set2', 'Gm1', 'Gm2', 'Pts', 'Gm#', 'TbSet',\n",
      "       'TB?', 'TBpt', 'Svr', 'Ret', 'Serving', '1st', '2nd', 'Notes',\n",
      "       '1stNoLet', '2ndNoLet', '1stSV', '2ndSV', '1stNoSV', '2ndNoSV', '1stIn',\n",
      "       '2ndIn', 'isRally1st', 'isRally2nd', 'Sv1', 'Sv2', 'Rally', 'isAce',\n",
      "       'isUnret', 'isRallyWinner', 'isForced', 'isUnforced', 'isDouble',\n",
      "       'rallyNoSpec', 'rallyNoError', 'rallyNoDirection', 'rallyLen',\n",
      "       'PtWinner', 'isSvrWinner', 'PtsAfter', 'GmW', 'Gm1.1', 'Gm2.1', 'SetW',\n",
      "       'Set1.1', 'Set2.1', 'RevTB', 'TBrev', 'rallyCount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Ensure the file exists in the current directory or provide the correct path\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "file_path = 'data/charting-m-points.csv' \n",
    "\n",
    "data = pd.read_csv(file_path, encoding='latin1')\n",
    "\t\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Pt                    0\n",
      "Set1                  0\n",
      "Set2                  0\n",
      "Gm1                   0\n",
      "Gm2                   1\n",
      "Pts                   0\n",
      "Gm#                   1\n",
      "TB?                  75\n",
      "rallyLen              0\n",
      "match_id              0\n",
      "Svr                   0\n",
      "Ret                   0\n",
      "Serving             946\n",
      "Sv1                   0\n",
      "Sv2              205088\n",
      "isAce                 0\n",
      "isUnret              10\n",
      "isRallyWinner        10\n",
      "isForced             10\n",
      "isUnforced            0\n",
      "isDouble              0\n",
      "rallyNoError      44059\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dropped_features = [\"TbSet\", \"TBpt\", \"1st\", \"2nd\", \"Notes\", \"1stNoLet\", \"2ndNoLet\", \"1stSV\", \"2ndSV\", \"1stNoSV\", \"2ndNoSV\", \"1stIn\", \"2ndIn\", \"isRally1st\", \"isRally2nd\", \"Rally\", \"rallyNoSpec\", \"rallyNoDirection\", \"PtWinner\", \"isSvrWinner\", \"PtsAfter\", 'GmW', 'Gm1.1', 'Gm2.1', 'SetW', 'Set1.1', 'Set2.1', \"RevTB\", \"TBrev\", \"rallyCount\"]\n",
    "kept_features = [\"Pt\", \"Set1\", \"Set2\", \"Gm1\", \"Gm2\", \"Pts\", \"Gm#\", \"TB?\", \"rallyLen\"]\n",
    "processing_features = [\"match_id\", \"Svr\", \"Ret\", \"Serving\", \"Sv1\", \"Sv2\", \"isAce\", \"isUnret\",\n",
    "                       \"isRallyWinner\", \"isForced\", \"isUnforced\", \"isDouble\", \"rallyNoError\"]\n",
    "\n",
    "data = data.drop(columns=dropped_features, errors='ignore')\n",
    "kept_features_data = data[kept_features].copy()\n",
    "processing_features_data = data[processing_features].copy()\n",
    "data = pd.concat([kept_features_data, processing_features_data], axis=1)\n",
    "print(\"Missing values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "Pt                    0\n",
      "Set1                  0\n",
      "Set2                  0\n",
      "Gm1                   0\n",
      "Gm2                   1\n",
      "Pts                   0\n",
      "Gm#                   1\n",
      "TB?                  75\n",
      "rallyLen              0\n",
      "match_id              0\n",
      "Svr                   0\n",
      "Ret                   0\n",
      "Serving             946\n",
      "Sv1                   0\n",
      "Sv2              205088\n",
      "isAce                 0\n",
      "isUnret              10\n",
      "isRallyWinner        10\n",
      "isForced             10\n",
      "isUnforced            0\n",
      "isDouble              0\n",
      "rallyNoError      44059\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([kept_features_data, processing_features_data], axis=1)\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process deduced features and compount features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_player(data, target_player):\n",
    "    \"\"\"\n",
    "    Filters the data for rows where the target player is playing.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input dataset.\n",
    "    target_player (str): The name of the target player.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered dataset containing only rows where the target player is playing.\n",
    "    \"\"\"\n",
    "    # Select \"match_id\" where target player is playing\n",
    "    selected_match_ids = data.loc[data['Serving'] == target_player, 'match_id'].unique()\n",
    "\n",
    "    # Filter rows in data where \"match_id\" is in the selected match_ids\n",
    "    filtered_data = data[data['match_id'].isin(selected_match_ids)]\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "#In processing_data create \"Svr\" : 1 if the target player is serving, 0 if the target player is receiving\n",
    "def create_svr_column(data, target_player): \n",
    "    \"\"\"\n",
    "    Creates a new column \"Svr\" in the dataset indicating if the target player is serving.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input dataset.\n",
    "    target_player (str): The name of the target player.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated dataset with the new \"Svr\" column.\n",
    "    \"\"\"\n",
    "    data['Svr'] = np.where(data['Serving'] == target_player, 1, 0)\n",
    "    return data\n",
    "\n",
    "def align_score_to_target_perspective(df):\n",
    "    score_map = {'0': 0, '15': 1, '30': 2, '40': 3, 'AD': 4}\n",
    "    df[['server_score_raw', 'receiver_score_raw']] = df['Pts'].str.split('-', expand=True)\n",
    "    df['server_score'] = df['server_score_raw'].map(score_map)\n",
    "    df['receiver_score'] = df['receiver_score_raw'].map(score_map)\n",
    "\n",
    "    # Align to target player's perspective\n",
    "    flip_mask = df['Svr'] == 0\n",
    "    df.loc[flip_mask, ['server_score', 'receiver_score']] = df.loc[flip_mask, ['receiver_score', 'server_score']].values\n",
    "    df.loc[flip_mask, ['server_score_raw', 'receiver_score_raw']] = df.loc[flip_mask, ['receiver_score_raw', 'server_score_raw']].values\n",
    "\n",
    "    df.rename(columns={\n",
    "        'server_score': 'player_score',\n",
    "        'receiver_score': 'opponent_score'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_score_features(df):\n",
    "    df['is_deuce'] = ((df['player_score'] == 3) & (df['opponent_score'] == 3)).astype(int)\n",
    "    df['is_break_point'] = ((df['opponent_score'] >= 3) & (df['player_score'] < 3)).astype(int)\n",
    "    df['is_game_point'] = ((df['player_score'] >= 3) & (df['opponent_score'] < 3)).astype(int)\n",
    "    df['point_diff'] = df['player_score'] - df['opponent_score']\n",
    "    return df\n",
    "\n",
    "def enrich_match_context(df):\n",
    "    df['total_sets_played'] = df['Set1'] + df['Set2']\n",
    "    df['total_games_played'] = df['Gm1'] + df['Gm2']\n",
    "    df['is_tiebreak'] = df['TB?'].astype(int)\n",
    "    df['match_pressure_score'] = (\n",
    "        df['is_break_point'] + df['is_game_point'] + df['is_tiebreak'] + df['is_deuce']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def estimate_stamina(df):\n",
    "    df['rally_intensity'] = df.groupby(['Set1', 'Set2', 'Gm#'])['rallyLen'].transform('mean')\n",
    "    df['fatigue_index'] = (\n",
    "        df['total_sets_played'] * 2 +\n",
    "        df['total_games_played'] +\n",
    "        df['rallyLen'] / 10 +\n",
    "        df['is_tiebreak'] * 3\n",
    "    )\n",
    "    df['estimated_stamina'] = 1 / (1 + df['fatigue_index'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shot types to be mapped for our unforced error and winners array\n",
    "shot_types = [\n",
    "    'f', 'b',  # groundstrokes\n",
    "    'r', 's',  # slices\n",
    "    'v', 'z',  # volleys\n",
    "    'o', 'p',  # overheads\n",
    "    'u', 'y',  # drop shots\n",
    "    'l', 'm',  # lobs\n",
    "    'h', 'i',  # half-volleys\n",
    "    'j', 'k',  # swinging volleys\n",
    "    #'t', 'q'   # trick shots and unknown shots\n",
    "]\n",
    "\n",
    "# generate combinations with directions 1, 2, 3\n",
    "shot_vocab = {f\"{shot}{n}\": idx for idx, (shot, n) in enumerate(\n",
    "    (s, i) for s in shot_types for i in [1, 2, 3]\n",
    ")}\n",
    "\n",
    "\n",
    "def process_rally_data(df, shot_vocab):\n",
    "    sequence_data = []\n",
    "    direction_dict = {'1', '2', '3'}\n",
    "    serve_dict={'4','5','6'}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        isServe = row['Svr'] == 1\n",
    "        rally = str(row['rallyNoError'])\n",
    "\n",
    "        if pd.isna(rally) or len(rally) < 2:\n",
    "            continue\n",
    "\n",
    "        debug_logs = []\n",
    "        tokens = []\n",
    "\n",
    "        # First shot: allow 1–2 digits\n",
    "        match = re.match(r'^([a-zA-Z])(\\d+)', rally)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        first_letter = match.group(1)\n",
    "        digits = match.group(2)[:2]\n",
    "        first_token = first_letter + digits[0]\n",
    "\n",
    "        if first_token in shot_vocab:\n",
    "            tokens.append(first_token)\n",
    "        else:\n",
    "            continue\n",
    "        idx = len(match.group(0))\n",
    "\n",
    "        # Extract remaining shots\n",
    "        while idx + 2 <= len(rally):\n",
    "            segment = rally[idx:idx+3]\n",
    "            debug_logs.append(f\"[{idx}] Segment: '{segment}'\")\n",
    "\n",
    "            if re.match(r'^[a-zA-Z]\\d[a-zA-Z]$', segment):\n",
    "                token = segment[0] + segment[1]\n",
    "                debug_logs.append(f\"[{idx}] Pattern A: {segment}\")\n",
    "\n",
    "                if token in shot_vocab:\n",
    "                    tokens.append(token)\n",
    "\n",
    "                else:\n",
    "                    debug_logs.append(f\"[{idx}] ❌ Invalid token: {token}\")\n",
    "                    break\n",
    "                idx += 2\n",
    "\n",
    "            elif re.match(r'^[a-zA-Z]\\d\\d$', segment):\n",
    "                debug_logs.append(f\"[{idx}] Pattern B: {segment}\")\n",
    "                token1 = segment[0] + segment[1]\n",
    "                token2 = segment[0] + segment[2]\n",
    "\n",
    "                if segment[1] in direction_dict and token1 in shot_vocab:\n",
    "                    tokens.append(token1)\n",
    "\n",
    "                elif segment[2] in direction_dict and token2 in shot_vocab:\n",
    "                    tokens.append(token2)\n",
    "\n",
    "                else:\n",
    "                    debug_logs.append(f\"[{idx}] ❌ Invalid tokens: {token1}, {token2}\")\n",
    "                    break\n",
    "                idx += 3\n",
    "\n",
    "            elif re.match(r'^[a-zA-Z][a-zA-Z]\\d$', segment):\n",
    "                debug_logs.append(f\"[{idx}] Pattern C: {segment}\")\n",
    "                token = segment[1] + segment[2]\n",
    "\n",
    "                if token in shot_vocab:\n",
    "                    tokens.append(token)\n",
    "\n",
    "                else:\n",
    "                    debug_logs.append(f\"[{idx}] ❌ Invalid token: {token}\")\n",
    "                    break\n",
    "                idx += 3\n",
    "\n",
    "            else:\n",
    "                debug_logs.append(f\"[{idx}] Pattern D (Fallback): {segment}\")\n",
    "\n",
    "                if idx + 1 < len(rally):\n",
    "                    ch1, ch2 = rally[idx], rally[idx + 1]\n",
    "                    token = ch1 + ch2\n",
    "\n",
    "                    if ch1.isalpha() and ch2 in direction_dict and token in shot_vocab:\n",
    "                        tokens.append(token)\n",
    "                        idx += 2\n",
    "\n",
    "                    else:\n",
    "                        debug_logs.append(f\"[{idx}] ❌ Invalid fallback token: {token}\")\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        # Skip rallies with invalid parsing\n",
    "        if any(log.startswith(\"❌\") for log in debug_logs):\n",
    "            print(f\"\\n🔍 Invalid rally at row {row.name}: {rally}\")\n",
    "            for log in debug_logs:\n",
    "                print(log)\n",
    "            continue\n",
    "\n",
    "        # --- Serve Logic ---\n",
    "        if not pd.isna(row['Sv1']) and row['Sv1'][0] in serve_dict:\n",
    "            if not pd.isna(row['Sv2']) and row['Sv2'][0] in serve_dict:\n",
    "                full_rally = [row['Sv1'][0], row['Sv2'][0]] + tokens\n",
    "            else:\n",
    "                full_rally = ['0', row['Sv1'][0]] + tokens\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # --- Label Arrays ---\n",
    "        winner_array = np.zeros(48)\n",
    "        unforced_array = np.zeros(48)\n",
    "\n",
    "        final_shot = tokens[-1]\n",
    "        idx = shot_vocab.get(final_shot, None)\n",
    "        if idx is not None:\n",
    "            if row['isRallyWinner']:\n",
    "                winner_array[idx] += 1\n",
    "            elif row['isUnforced']:\n",
    "                unforced_array[idx] += 1\n",
    "\n",
    "        # --- 3-Shot Sequence Construction ---\n",
    "        i = 0 if isServe else 1\n",
    "        while i + 3 < len(full_rally):\n",
    "            new_row = row.to_dict()\n",
    "            new_row['shot1'] = full_rally[i]\n",
    "            new_row['shot2'] = full_rally[i + 1]\n",
    "            new_row['shot3'] = full_rally[i + 2]\n",
    "            new_row['shot4'] = full_rally[i + 3]\n",
    "            new_row['winner_array'] = winner_array.copy()\n",
    "            new_row['unforced_array'] = unforced_array.copy()\n",
    "            sequence_data.append(new_row)\n",
    "            i += 2\n",
    "\n",
    "    return pd.DataFrame(sequence_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_column_types(df):\n",
    "    expected_types = {\n",
    "        'Pt': 'int64', 'Set1': 'int64', 'Set2': 'int64', 'Gm1': 'int64',\n",
    "        'Gm2': 'float64', 'Pts': 'object', 'Gm#': 'object',\n",
    "        'TB?': 'float64', 'rallyLen': 'int64'\n",
    "    }\n",
    "    for col, expected in expected_types.items():\n",
    "        if col in df.columns and df[col].dtype != expected:\n",
    "            print(f\"Column {col} has type {df[col].dtype}, expected {expected}\")\n",
    "    return df\n",
    "\n",
    "def validate_score_format(df):\n",
    "    valid_scores = {'0', '15', '30', '40', 'AD'}\n",
    "    df[['server_score_raw', 'receiver_score_raw']] = df['Pts'].str.split('-', expand=True)\n",
    "    invalid_scores = df[\n",
    "        (~df['server_score_raw'].isin(valid_scores)) |\n",
    "        (~df['receiver_score_raw'].isin(valid_scores))\n",
    "    ]\n",
    "    if not invalid_scores.empty:\n",
    "        print(\"Invalid score entries found:\")\n",
    "        print(invalid_scores[['Pts']].drop_duplicates())\n",
    "    return df[~df.index.isin(invalid_scores.index)]\n",
    "\n",
    "def validate_set_game_counts(df):\n",
    "    invalid_sets = df[(df['Set1'] > 3) | (df['Set2'] > 3)]\n",
    "    invalid_games = df[(df['Gm1'] > 7) | (df['Gm2'] > 7)]\n",
    "    df = df.drop(invalid_sets.index.union(invalid_games.index))\n",
    "    return df\n",
    "\n",
    "def validate_tennis_data(df):\n",
    "    df = validate_column_types(df)\n",
    "    df = validate_score_format(df)\n",
    "    df = validate_set_game_counts(df)\n",
    "    return df\n",
    "\n",
    "def preprocess_numeric_columns(df):\n",
    "    # Clean Gm# to integer\n",
    "    if 'Gm#' in df.columns:\n",
    "        df['Gm#'] = df['Gm#'].apply(lambda x: int(re.match(r'\\d+', str(x)).group()) if re.match(r'\\d+', str(x)) else 0)\n",
    "\n",
    "    # Ensure score columns are integers where applicable\n",
    "    score_cols = ['Set1', 'Set2', 'Gm1', 'Gm2']\n",
    "    for col in score_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Rally length cleanup\n",
    "    if 'rallyLen' in df.columns:\n",
    "        df['rallyLen'] = pd.to_numeric(df['rallyLen'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    df['TB?'] = df['TB?'].fillna(0)\n",
    "    df.dropna(subset=['Gm2', 'Gm#'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acutual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid score entries found:\n",
      "          Pts\n",
      "7108      0-1\n",
      "7109      1-1\n",
      "7110      1-2\n",
      "7111      2-2\n",
      "7112      2-3\n",
      "...       ...\n",
      "266584  16-17\n",
      "266585  17-17\n",
      "266586  17-18\n",
      "266587  18-18\n",
      "266588  19-18\n",
      "\n",
      "[77 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_tennis_data(data, target_player=\"RF\"):\n",
    "    df = data.copy()\n",
    "    df = filter_data_by_player(df, target_player)\n",
    "    df = create_svr_column(df, target_player)\n",
    "    df = validate_tennis_data(df)    \n",
    "    df = align_score_to_target_perspective(df)\n",
    "    df = process_rally_data(df, shot_vocab=shot_vocab)\n",
    "    df = enrich_score_features(df)\n",
    "    df = enrich_match_context(df)\n",
    "    df = estimate_stamina(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = preprocess_numeric_columns(df)\n",
    "    df = df.drop(columns=processing_features, errors='ignore')\n",
    "    return df\n",
    "\n",
    "processed_data = process_tennis_data(data, target_player=\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69077 entries, 0 to 69076\n",
      "Data columns (total 30 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Pt                    69077 non-null  int64  \n",
      " 1   Set1                  69077 non-null  int32  \n",
      " 2   Set2                  69077 non-null  int32  \n",
      " 3   Gm1                   69077 non-null  int32  \n",
      " 4   Gm2                   69077 non-null  int32  \n",
      " 5   Pts                   69077 non-null  object \n",
      " 6   Gm#                   69077 non-null  int64  \n",
      " 7   TB?                   69077 non-null  float64\n",
      " 8   rallyLen              69077 non-null  int32  \n",
      " 9   server_score_raw      69077 non-null  object \n",
      " 10  receiver_score_raw    69077 non-null  object \n",
      " 11  player_score          69077 non-null  float64\n",
      " 12  opponent_score        69077 non-null  float64\n",
      " 13  shot1                 69077 non-null  object \n",
      " 14  shot2                 69077 non-null  object \n",
      " 15  shot3                 69077 non-null  object \n",
      " 16  shot4                 69077 non-null  object \n",
      " 17  winner_array          69077 non-null  object \n",
      " 18  unforced_array        69077 non-null  object \n",
      " 19  is_deuce              69077 non-null  int32  \n",
      " 20  is_break_point        69077 non-null  int32  \n",
      " 21  is_game_point         69077 non-null  int32  \n",
      " 22  point_diff            69077 non-null  float64\n",
      " 23  total_sets_played     69077 non-null  int64  \n",
      " 24  total_games_played    69077 non-null  float64\n",
      " 25  is_tiebreak           69077 non-null  int32  \n",
      " 26  match_pressure_score  69077 non-null  int32  \n",
      " 27  rally_intensity       69077 non-null  float64\n",
      " 28  fatigue_index         69077 non-null  float64\n",
      " 29  estimated_stamina     69077 non-null  float64\n",
      "dtypes: float64(8), int32(10), int64(3), object(9)\n",
      "memory usage: 13.2+ MB\n"
     ]
    }
   ],
   "source": [
    "processed_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in shot1: ['b1' 'b2' 'b3' 'f1' 'f2' 'f3' 'h1' 'h2' 'h3' 'i1' 'i2' 'i3' 'j1' 'j2'\n",
      " 'j3' 'k1' 'k3' 'l1' 'l2' 'l3' 'm1' 'm2' 'm3' 'o1' 'o2' 'o3' 'p1' 'p2'\n",
      " 'p3' 'r1' 'r2' 'r3' 's1' 's2' 's3' 'u1' 'u2' 'u3' 'v1' 'v2' 'v3' 'y1'\n",
      " 'y2' 'y3' 'z1' 'z2' 'z3']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels in shot1:\", np.unique(processed_data['shot4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup dataset\n",
    "processed_data.to_csv('data/processed_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_array_column(df, column):\n",
    "    return df[column].apply(lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else np.zeros(48))\n",
    "\n",
    "processed_data['winner_array'] = parse_array_column(processed_data, 'winner_array')\n",
    "processed_data['unforced_array'] = parse_array_column(processed_data, 'unforced_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outliers(df, cols, lower=0.01, upper=0.99):\n",
    "    for col in cols:\n",
    "        q_low = df[col].quantile(lower)\n",
    "        q_high = df[col].quantile(upper)\n",
    "        df[col] = df[col].clip(q_low, q_high)\n",
    "    return df\n",
    "\n",
    "numerical_cols = [\n",
    "    'player_score', 'opponent_score', 'point_diff', 'rallyLen',\n",
    "    'rally_intensity', 'fatigue_index', 'estimated_stamina',\n",
    "    'total_sets_played', 'total_games_played', 'match_pressure_score'\n",
    "]\n",
    "processed_data = clip_outliers(processed_data, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "shot_columns = ['shot1', 'shot2', 'shot3', 'shot4']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in shot_columns:\n",
    "    processed_data[col + '_encoded'] = label_encoder.fit_transform(processed_data[col])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "processed_data[numerical_cols] = scaler.fit_transform(processed_data[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = ['is_deuce', 'is_break_point', 'is_game_point', 'is_tiebreak']\n",
    "\n",
    "X_seq = []\n",
    "features = processed_data[numerical_cols + binary_cols].values\n",
    "shot1 = processed_data['shot1_encoded'].values\n",
    "shot2 = processed_data['shot2_encoded'].values\n",
    "y = processed_data['target_class'].values\n",
    "\n",
    "for f, s1, s2 in zip(features, shot1, shot2):\n",
    "    t1 = np.concatenate(([s1], f))\n",
    "    t2 = np.concatenate(([s2], f))\n",
    "    X_seq.append([t1, t2])\n",
    "\n",
    "X_seq = np.array(X_seq)  # shape: (samples, 2, input_size)\n",
    "print(\"RNN input shape:\", X_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word embeddings with full dataset\n",
    "With our model, label encoding has worked better than word embeddings, so we will hide this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first pre-process the full dataset (all players) with the same cleaning process that we need for the Rafael Nadal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the processing to not take into account Svr, as we are looking at all players\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def process_full_rally_data(df, shot_vocab):\n",
    "    sequence_data = []\n",
    "    direction_dict = {'1', '2', '3'}\n",
    "    serve_dict={'4','5','6'}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        isServe = row['Svr'] == 1\n",
    "        rally = str(row['rallyNoError'])\n",
    "\n",
    "        if pd.isna(rally) or len(rally) < 2:\n",
    "            continue\n",
    "\n",
    "        debug_logs = []\n",
    "        tokens = []\n",
    "\n",
    "        # First shot: allow 1–2 digits\n",
    "        match = re.match(r'^([a-zA-Z])(\\d+)', rally)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        first_letter = match.group(1)\n",
    "        digits = match.group(2)[:2]\n",
    "        first_token = first_letter + digits[0]\n",
    "\n",
    "        if first_token in shot_vocab:\n",
    "            tokens.append(first_token)\n",
    "        else:\n",
    "            continue\n",
    "        idx = len(match.group(0))\n",
    "\n",
    "        # Extract remaining shots\n",
    "        while idx + 2 <= len(rally):\n",
    "            segment = rally[idx:idx+3]\n",
    "            debug_logs.append(f\"[{idx}] Segment: '{segment}'\")\n",
    "\n",
    "            if re.match(r'^[a-zA-Z]\\d[a-zA-Z]$', segment):\n",
    "                token = segment[0] + segment[1]\n",
    "                debug_logs.append(f\"[{idx}] Pattern A: {segment}\")\n",
    "\n",
    "                if token in shot_vocab:\n",
    "                    tokens.append(token)\n",
    "\n",
    "                else:\n",
    "                    debug_logs.append(f\"[{idx}] ❌ Invalid token: {token}\")\n",
    "                    break\n",
    "                idx += 2\n",
    "\n",
    "            elif re.match(r'^[a-zA-Z]\\d\\d$', segment):\n",
    "                debug_logs.append(f\"[{idx}] Pattern B: {segment}\")\n",
    "                token1 = segment[0] + segment[1]\n",
    "                token2 = segment[0] + segment[2]\n",
    "\n",
    "                if segment[1] in direction_dict and token1 in shot_vocab:\n",
    "                    tokens.append(token1)\n",
    "\n",
    "                elif segment[2] in direction_dict and token2 in shot_vocab:\n",
    "                    tokens.append(token2)\n",
    "\n",
    "                else:\n",
    "                    debug_logs.append(f\"[{idx}] ❌ Invalid tokens: {token1}, {token2}\")\n",
    "                    break\n",
    "                idx += 3\n",
    "\n",
    "            elif re.match(r'^[a-zA-Z][a-zA-Z]\\d$', segment):\n",
    "                debug_logs.append(f\"[{idx}] Pattern C: {segment}\")\n",
    "                token = segment[1] + segment[2]\n",
    "\n",
    "                if token in shot_vocab:\n",
    "                    tokens.append(token)\n",
    "\n",
    "                else:\n",
    "                    debug_logs.append(f\"[{idx}] ❌ Invalid token: {token}\")\n",
    "                    break\n",
    "                idx += 3\n",
    "\n",
    "            else:\n",
    "                debug_logs.append(f\"[{idx}] Pattern D (Fallback): {segment}\")\n",
    "\n",
    "                if idx + 1 < len(rally):\n",
    "                    ch1, ch2 = rally[idx], rally[idx + 1]\n",
    "                    token = ch1 + ch2\n",
    "\n",
    "                    if ch1.isalpha() and ch2 in direction_dict and token in shot_vocab:\n",
    "                        tokens.append(token)\n",
    "                        idx += 2\n",
    "\n",
    "                    else:\n",
    "                        debug_logs.append(f\"[{idx}] ❌ Invalid fallback token: {token}\")\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        # Skip rallies with invalid parsing\n",
    "        if any(log.startswith(\"❌\") for log in debug_logs):\n",
    "            print(f\"\\n🔍 Invalid rally at row {row.name}: {rally}\")\n",
    "            for log in debug_logs:\n",
    "                print(log)\n",
    "            continue\n",
    "\n",
    "        # --- Serve Logic --- CHANGED FOR FULL RALLY\n",
    "        serve_tokens = []\n",
    "\n",
    "        if pd.notna(row['Sv1']) and row['Sv1'][0] in serve_dict:\n",
    "            serve_tokens.append(row['Sv1'][0])\n",
    "            \n",
    "            if pd.notna(row['Sv2']) and row['Sv2'][0] in serve_dict:\n",
    "                serve_tokens.append(row['Sv2'][0])\n",
    "\n",
    "            full_rally = serve_tokens + tokens\n",
    "        else:\n",
    "            continue  # skip rally if no valid Sv1\n",
    "\n",
    "\n",
    "        # --- Label Arrays ---\n",
    "        winner_array = np.zeros(48)\n",
    "        unforced_array = np.zeros(48)\n",
    "\n",
    "        final_shot = tokens[-1]\n",
    "        idx = shot_vocab.get(final_shot, None)\n",
    "        if idx is not None:\n",
    "            if row['isRallyWinner']:\n",
    "                winner_array[idx] += 1\n",
    "            elif row['isUnforced']:\n",
    "                unforced_array[idx] += 1\n",
    "\n",
    "        # --- 3-Shot Sequence Construction ---\n",
    "        i = 0 if isServe else 1\n",
    "        while i + 3 < len(full_rally):\n",
    "            new_row = row.to_dict()\n",
    "            new_row['shot1'] = full_rally[i]\n",
    "            new_row['shot2'] = full_rally[i + 1]\n",
    "            new_row['shot3'] = full_rally[i + 2]\n",
    "            new_row['shot4'] = full_rally[i + 3]\n",
    "            new_row['winner_array'] = winner_array.copy()\n",
    "            new_row['unforced_array'] = unforced_array.copy()\n",
    "            sequence_data.append(new_row)\n",
    "            i += 2\n",
    "\n",
    "    return pd.DataFrame(sequence_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of Unique Matches Found: 2053\n",
      "Number of Rows Selected: 331720\n",
      "Sum of Rally Lengths: 1258899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_30684\\587475941.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_full_data['TB?'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Filtered dataset preview:\n",
      "   rallyLen shot1 shot2 shot3 shot4  winner_0  winner_1  winner_2  winner_3  \\\n",
      "0         6     0     6    b1    f2       0.0       0.0       0.0       0.0   \n",
      "1         6    b1    f2    f1    b1       0.0       0.0       0.0       0.0   \n",
      "2         6    f1    b1    f1    b1       0.0       0.0       0.0       0.0   \n",
      "3        15     0     6    f3    f2       0.0       0.0       0.0       0.0   \n",
      "4        15    f3    f2    f1    b2       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   winner_4  ...  unforced_10  unforced_11  unforced_12  unforced_13  \\\n",
      "0       0.0  ...          0.0          0.0          0.0          0.0   \n",
      "1       0.0  ...          0.0          0.0          0.0          0.0   \n",
      "2       0.0  ...          0.0          0.0          0.0          0.0   \n",
      "3       0.0  ...          0.0          0.0          0.0          0.0   \n",
      "4       0.0  ...          0.0          0.0          0.0          0.0   \n",
      "\n",
      "   unforced_14  unforced_16  unforced_17  unforced_26  unforced_27  \\\n",
      "0          0.0          0.0          0.0          0.0          0.0   \n",
      "1          0.0          0.0          0.0          0.0          0.0   \n",
      "2          0.0          0.0          0.0          0.0          0.0   \n",
      "3          0.0          0.0          0.0          0.0          0.0   \n",
      "4          0.0          0.0          0.0          0.0          0.0   \n",
      "\n",
      "   unforced_29  \n",
      "0          0.0  \n",
      "1          0.0  \n",
      "2          0.0  \n",
      "3          0.0  \n",
      "4          0.0  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "\n",
      "Remaining columns: ['rallyLen', 'shot1', 'shot2', 'shot3', 'shot4', 'winner_0', 'winner_1', 'winner_2', 'winner_3', 'winner_4', 'winner_5', 'winner_12', 'winner_13', 'winner_14', 'winner_15', 'winner_17', 'winner_18', 'winner_20', 'winner_24', 'winner_26', 'winner_27', 'winner_38', 'winner_42', 'winner_43', 'winner_44', 'unforced_0', 'unforced_1', 'unforced_2', 'unforced_3', 'unforced_4', 'unforced_5', 'unforced_9', 'unforced_10', 'unforced_11', 'unforced_12', 'unforced_13', 'unforced_14', 'unforced_16', 'unforced_17', 'unforced_26', 'unforced_27', 'unforced_29']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "processing_full_data = data.copy()\n",
    "\n",
    "# Print basic stats\n",
    "print(f\"Number of Unique Matches Found: {processing_full_data['match_id'].nunique()}\")\n",
    "print(f\"Number of Rows Selected: {len(processing_full_data)}\")\n",
    "\n",
    "if 'rallyLen' in processing_full_data.columns:\n",
    "    print(f\"Sum of Rally Lengths: {processing_full_data['rallyLen'].sum()}\")\n",
    "else:\n",
    "    print(\"Column 'rallyLen' not found in the dataset.\")\n",
    "\n",
    "# Process rallies\n",
    "sequence_full_data = process_rally_data(processing_full_data, shot_vocab=shot_vocab)\n",
    "\n",
    "# Sample if too large\n",
    "if len(sequence_full_data) > 500_000:\n",
    "    sequence_full_data = sequence_full_data.sample(n=500_000, random_state=42)\n",
    "    print(\"Sampled down to 500,000 sequences for embedding training.\")\n",
    "\n",
    "# Drop irrelevant columns if defined in processing_features\n",
    "processed_full_data = sequence_full_data.drop(columns=processing_features, errors='ignore')\n",
    "\n",
    "# Convert 'Pts' to Pts1 and Pts2\n",
    "processed_full_data[['Pts1', 'Pts2']] = processed_full_data['Pts'].apply(\n",
    "    lambda x: pd.Series(split_pts(str(x)))\n",
    ")\n",
    "processed_full_data.drop('Pts', axis=1, inplace=True)\n",
    "\n",
    "# Clean Gm# column\n",
    "processed_full_data['Gm#'] = processed_full_data['Gm#'].apply(\n",
    "    lambda x: int(re.match(r'\\d+', str(x)).group()) if re.match(r'\\d+', str(x)) else 0\n",
    ")\n",
    "\n",
    "# Fill missing tiebreak indicator\n",
    "processed_full_data['TB?'].fillna(0, inplace=True)\n",
    "\n",
    "# Drop rows with missing game counts\n",
    "processed_full_data.dropna(subset=['Gm2', 'Gm#'], inplace=True)\n",
    "\n",
    "# Unpack winner/unforced arrays\n",
    "winner_df = pd.DataFrame(processed_full_data['winner_array'].tolist(), index=processed_full_data.index)\n",
    "winner_df.columns = [f'winner_{i}' for i in range(winner_df.shape[1])]\n",
    "unforced_df = pd.DataFrame(processed_full_data['unforced_array'].tolist(), index=processed_full_data.index)\n",
    "unforced_df.columns = [f'unforced_{i}' for i in range(unforced_df.shape[1])]\n",
    "\n",
    "processed_full_data.drop(columns=['winner_array', 'unforced_array'], inplace=True)\n",
    "processed_full_data = pd.concat([processed_full_data, winner_df, unforced_df], axis=1)\n",
    "# ---------------------------\n",
    "# Drop columns not deemed important by XGBoost\n",
    "# ---------------------------\n",
    "important_features = set([\n",
    "    'rallyLen', 'shot3',\n",
    "    'winner_0', 'winner_1', 'winner_2', 'winner_3', 'winner_4', 'winner_5',\n",
    "    'winner_12', 'winner_13', 'winner_14', 'winner_15', 'winner_17', 'winner_18',\n",
    "    'winner_20', 'winner_24', 'winner_26', 'winner_27', 'winner_38', 'winner_42', 'winner_43', 'winner_44',\n",
    "    'unforced_0', 'unforced_1', 'unforced_2', 'unforced_3', 'unforced_4', 'unforced_5',\n",
    "    'unforced_9', 'unforced_10', 'unforced_11', 'unforced_12', 'unforced_13', 'unforced_14',\n",
    "    'unforced_16', 'unforced_17', 'unforced_26', 'unforced_27', 'unforced_29'\n",
    "])\n",
    "\n",
    "# Always retain shot1, shot2, shot4 for training and prediction\n",
    "must_have = {'shot1', 'shot2', 'shot4'}\n",
    "columns_to_keep = list(important_features.union(must_have))\n",
    "\n",
    "# Filter only the necessary columns for modeling\n",
    "filtered_full_data = processed_full_data[[col for col in processed_full_data.columns if col in columns_to_keep]]\n",
    "\n",
    "print(\"\\n✅ Filtered dataset preview:\")\n",
    "print(filtered_full_data.head())\n",
    "print(\"\\nRemaining columns:\", filtered_full_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1831\n",
      "Epoch 2, Loss: 2.0734\n",
      "Epoch 3, Loss: 2.0574\n",
      "Epoch 4, Loss: 2.0509\n",
      "Epoch 5, Loss: 2.0472\n",
      "Epoch 6, Loss: 2.0444\n",
      "Epoch 7, Loss: 2.0423\n",
      "Epoch 8, Loss: 2.0407\n",
      "Epoch 9, Loss: 2.0392\n",
      "Epoch 10, Loss: 2.0379\n",
      "✅ Saved pretrained embeddings to pretrained_shot_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Encode shot tokens\n",
    "shot_encoder = LabelEncoder()\n",
    "all_shots = pd.concat([processed_full_data['shot1'], processed_full_data['shot2'], processed_full_data['shot3'], processed_full_data['shot4']])\n",
    "shot_encoder.fit(all_shots.astype(str))\n",
    "\n",
    "processed_full_data['shot1_enc'] = shot_encoder.transform(processed_full_data['shot1'].astype(str))\n",
    "processed_full_data['shot2_enc'] = shot_encoder.transform(processed_full_data['shot2'].astype(str))\n",
    "processed_full_data['shot3_enc'] = shot_encoder.transform(processed_full_data['shot3'].astype(str))\n",
    "processed_full_data['shot4_enc'] = shot_encoder.transform(processed_full_data['shot4'].astype(str))\n",
    "\n",
    "num_classes = len(shot_encoder.classes_)\n",
    "\n",
    "# Step 2: Prepare training tensors\n",
    "X = processed_full_data[['shot1_enc', 'shot2_enc', 'shot3_enc']].values\n",
    "y = processed_full_data['shot4_enc'].values\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.1, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=128)\n",
    "\n",
    "# Step 3: Define LSTM model for embedding pretraining\n",
    "class ShotEmbeddingTrainer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super(ShotEmbeddingTrainer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, 2, emb_dim]\n",
    "        _, (hn, _) = self.lstm(embedded)  # hn: [1, batch, hidden]\n",
    "        output = self.fc(hn.squeeze(0))  # [batch, num_classes]\n",
    "        return output\n",
    "\n",
    "# Step 4: Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ShotEmbeddingTrainer(\n",
    "    vocab_size=num_classes,\n",
    "    embedding_dim=16,\n",
    "    hidden_size=64,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Step 5: Save the embedding layer\n",
    "embedding_weights = model.embedding.weight.data.cpu().clone()\n",
    "torch.save(embedding_weights, \"pretrained_shot_embeddings.pt\")\n",
    "print(\"✅ Saved pretrained embeddings to pretrained_shot_embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('shot_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(shot_encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNLSTMShotPredictor(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(CNNLSTMShotPredictor, self).__init__()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=input_size,   # each feature over time\n",
    "            out_channels=64,\n",
    "            kernel_size=2,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,           # output of conv1d per timestep\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False      # match your best model\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, input_size] → [B, input_size, T]\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply 1D CNN\n",
    "        x = self.conv1d(x)          # [B, 64, T]\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Back to [B, T, 64]\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply LSTM\n",
    "        lstm_out, (hidden, _) = self.lstm(x)  # hidden: [num_layers, B, hidden_size]\n",
    "        final_hidden = hidden[-1]             # [B, hidden_size]\n",
    "        final_hidden = self.dropout(final_hidden)\n",
    "\n",
    "        return self.classifier(final_hidden)  # [B, num_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with pre-trained embeddings - NOT USED, LABEL ENCODING PERFORMED BETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rally vocab size: 48\n",
      "Original embedding shape: torch.Size([52, 16])\n",
      "Trimmed embedding shape: torch.Size([48, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_30684\\3645131874.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_weights = torch.load(\"pretrained_shot_embeddings.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete.\n",
      "Train shape: torch.Size([28625, 3, 107]) Target shape: torch.Size([28625])\n",
      "Num classes: 48\n"
     ]
    }
   ],
   "source": [
    "# # training with pre-trained embeddings\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import pickle\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # --- Define valid rally tokens (exclude serves like '0', '4', '5', '6') ---\n",
    "# rally_tokens = sorted([tok for tok in shot_vocab.keys() if tok[0] not in {'0', '4', '5', '6'}])\n",
    "# print(f\"Rally vocab size: {len(rally_tokens)}\")\n",
    "\n",
    "# # --- Rebuild LabelEncoder (rally-only) ---\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.classes_ = np.array(rally_tokens)\n",
    "\n",
    "# # --- Load pretrained encoder and embedding ---\n",
    "# with open(\"shot_encoder.pkl\", \"rb\") as f:\n",
    "#     full_encoder = pickle.load(f)\n",
    "\n",
    "# pretrained_weights = torch.load(\"pretrained_shot_embeddings.pt\")\n",
    "# print(\"Original embedding shape:\", pretrained_weights.shape)  # e.g. [52, 16]\n",
    "\n",
    "# # --- Map rally tokens to original indices and trim embedding ---\n",
    "# token_to_index = {tok: np.where(full_encoder.classes_ == tok)[0][0] for tok in rally_tokens}\n",
    "# trimmed_embeddings = torch.stack([pretrained_weights[token_to_index[tok]] for tok in rally_tokens])\n",
    "# print(\"Trimmed embedding shape:\", trimmed_embeddings.shape)  # Should be [48, 16]\n",
    "\n",
    "# # --- Preprocess your data ---\n",
    "# processed_data_rnn = processed_data.copy()\n",
    "\n",
    "# # Unpack winner/unforced arrays\n",
    "# winner_df = pd.DataFrame(processed_data_rnn['winner_array'].tolist(), index=processed_data_rnn.index)\n",
    "# winner_df.columns = [f'winner_{i}' for i in range(winner_df.shape[1])]\n",
    "# unforced_df = pd.DataFrame(processed_data_rnn['unforced_array'].tolist(), index=processed_data_rnn.index)\n",
    "# unforced_df.columns = [f'unforced_{i}' for i in range(unforced_df.shape[1])]\n",
    "\n",
    "# processed_data_rnn.drop(columns=['winner_array', 'unforced_array'], inplace=True)\n",
    "# processed_data_rnn = pd.concat([processed_data_rnn, winner_df, unforced_df], axis=1)\n",
    "\n",
    "# # --- Filter out any rows with serve tokens in shot1, shot2, shot3 or shot4 ---\n",
    "# def is_rally_token(tok):\n",
    "#     return isinstance(tok, str) and tok[0] not in {'0', '4', '5', '6'}\n",
    "\n",
    "# mask = (\n",
    "#     processed_data_rnn['shot1'].apply(is_rally_token) &\n",
    "#     processed_data_rnn['shot2'].apply(is_rally_token) &\n",
    "#     processed_data_rnn['shot3'].apply(is_rally_token) &\n",
    "#     processed_data_rnn['shot4'].apply(is_rally_token)\n",
    "# )\n",
    "# processed_data_rnn = processed_data_rnn[mask].copy()\n",
    "\n",
    "# # --- Encode shots using the new label encoder ---\n",
    "# processed_data_rnn['shot1'] = label_encoder.transform(processed_data_rnn['shot1'])\n",
    "# processed_data_rnn['shot2'] = label_encoder.transform(processed_data_rnn['shot2'])\n",
    "# processed_data_rnn['shot3'] = label_encoder.transform(processed_data_rnn['shot3'])\n",
    "# processed_data_rnn['shot4'] = label_encoder.transform(processed_data_rnn['shot4'])\n",
    "\n",
    "# # --- keep xgboost columns only---\n",
    "\n",
    "# important_features = set([\n",
    "#     'rallyLen', 'shot3',\n",
    "#     'winner_0', 'winner_1', 'winner_2', 'winner_3', 'winner_4', 'winner_5',\n",
    "#     'winner_12', 'winner_13', 'winner_14', 'winner_15', 'winner_17', 'winner_18',\n",
    "#     'winner_20', 'winner_24', 'winner_26', 'winner_27', 'winner_38', 'winner_42', 'winner_43', 'winner_44',\n",
    "#     'unforced_0', 'unforced_1', 'unforced_2', 'unforced_3', 'unforced_4', 'unforced_5',\n",
    "#     'unforced_9', 'unforced_10', 'unforced_11', 'unforced_12', 'unforced_13', 'unforced_14',\n",
    "#     'unforced_16', 'unforced_17', 'unforced_26', 'unforced_27', 'unforced_29'\n",
    "# ])\n",
    "\n",
    "# # Always retain shot1, shot2, shot4 for training and prediction\n",
    "# must_have = {'shot1', 'shot2', 'shot4'}\n",
    "# columns_to_keep = list(important_features.union(must_have))\n",
    "\n",
    "# # Filter only the necessary columns for modeling\n",
    "# filtered_full_data = processed_full_data[[col for col in processed_full_data.columns if col in columns_to_keep]]\n",
    "\n",
    "# # --- Define features and target ---\n",
    "# target = processed_data_rnn['shot4']\n",
    "# features = processed_data_rnn.drop(columns=['shot4'])\n",
    "\n",
    "# # Sanity check: target values must be in [0, 47]\n",
    "# assert target.min() >= 0 and target.max() < len(rally_tokens), \"Target labels out of bounds\"\n",
    "\n",
    "# # --- Build sequences: [shot1 + context], [shot2 + context] ---\n",
    "# context_cols = [col for col in features.columns if col not in ['shot1', 'shot2', 'shot3']]\n",
    "# X_sequences = []\n",
    "\n",
    "# for _, row in features.iterrows():\n",
    "#     context = row[context_cols].values.astype(np.float32)\n",
    "#     shot1 = np.insert(context, 0, row['shot1'])\n",
    "#     shot2 = np.insert(context, 0, row['shot2'])\n",
    "#     shot3 = np.insert(context, 0, row['shot3'])\n",
    "#     X_sequences.append(np.stack([shot1, shot2, shot3]))\n",
    "\n",
    "# # --- Normalize the features ---\n",
    "# X_array = np.array(X_sequences).reshape(-1, X_sequences[0].shape[1])\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled_flat = scaler.fit_transform(X_array)\n",
    "# X_scaled = X_scaled_flat.reshape(len(X_sequences), 3, -1)\n",
    "\n",
    "# # --- Convert to torch tensors ---\n",
    "# X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(target.values, dtype=torch.long)\n",
    "\n",
    "# # --- Train/test split ---\n",
    "# # Split train further into train + val (e.g., 60% train, 20% val, 20% test)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)\n",
    "\n",
    "# # --- Save the encoder and embeddings for model use ---\n",
    "# with open(\"trimmed_shot_encoder.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(label_encoder, f)\n",
    "\n",
    "# torch.save(trimmed_embeddings, \"trimmed_shot_embeddings.pt\")\n",
    "\n",
    "# print(\"✅ Preprocessing complete.\")\n",
    "# print(\"Train shape:\", X_train.shape, \"Target shape:\", y_train.shape)\n",
    "# print(\"Num classes:\", len(label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpCK1Lt7iODa7s+5h4SExo",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
